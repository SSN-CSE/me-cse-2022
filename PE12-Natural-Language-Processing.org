* <<<CP1224>>> NATURAL LANGUAGE PROCESSING
:Properties:
:author:  B Senthil Kumar, D Thenmozhi
:date: 11 May 2022
:end:

#+startup: showall

{{{credits}}}
|L|T|P|C|
|3|0|0|3|

** Course Objectives
- To learn the language models.
- To understand the levels of knowledge in language processing.
- To understand sequence processing of text. 
- To explore text processing using Python.

{{{unit}}}
|Unit I |Introduction|8| 
Origins and challenges of NLP -- Knowledge in language processing -- Ambiguity
-- NLP applications; Text Normalization -- N-grams -- Evaluation -- Sampling -- 
Generalization -- Smoothing


{{{unit}}}
|Unit II|Vector Semantics and Neural Language Modeling|9|
Vector Semantics: Words and vectors -- Cosine similarity -- TF-IDF -- PPMI -- Word2Vec -- Semantic properties 
of embeddings -- Evaluating vector model; Neural language model: Feedforward networks for NLP -- 
Feedforward Neural Language Modeling -- Training the neural language model -- RNN as Language Model


{{{unit}}}
|Unit III|Word Level and Syntactic Analysis |10| 
English Word Classes -- Part-of-Speech Tagging -- HMM PoS Tagging -- Context-free grammar -- 
Grammar rules for Engligh -- Treebanks -- Constituency Parsing: Ambiguity -- CKY Parsing -- 
Neural Constituency Parsing; Dependency Parsing: Dependency Relations -- Formalisms -- 
Transition-based and Graph-based Dependency Parsing;


{{{unit}}}
|Unit IV|Meaning Representation |8| 
The representation of Meaning: Computational Desiderata for Representations -- Model-Theoretic Semantics -- 
First-Order Logic -- Description Logics


{{{unit}}}
|Unit V|Semantic Analysis|9|
Word Senses -- Senses Relations -- WordNet -- Word Sense Disambiguation -- Feature-based WSD -- Knowledge-based WSD 
-- Lexical Semantics -- Semantic Roles -- Diathesis Alternations -- Problems with Thematic Roles -- 
Proposition Bank -- FrameNet -- Semantic Role Labeling


\hfill *Total: 45*

** Course Outcomes
After the completion of this course, students will be able to: 
- Apply the vector semantics in language modeling (K3)
- Apply the levels of knowledge in language processing (K3)
- Explain the Sequence processing with ML/Neural networks. (K2)
- Apply Python for Text Analysis (K3)
 
 Suggested Exercise:
 1) Implementation of Tokenizer, n-grams
 2) Implementation of Text vectorizer, BoW, Tf-idf
 3) Use Word2Vec, GloVe vectors for downstream tasks
 4) Implement PoS Tagging - sequence labeling
 5) Apply Dependency parsing and parse the sentence
 5) Implement Text classification using deep neural networks
 6) Word Similarity
 
     
** References
1. Daniel Jurafsky and James H Martin, “Speech and Language Processing: An introduction to Natural Language Processing, Computational Linguistics and Speech Recognition”,2nd Edition, Prentice Hall, 2008.
2. Tanveer Siddiqui, U.S. Tiwary, “Natural Language Processing and Information Retrieval”, Oxford University Press, 2008.
3. Steven Bird, Ewan Klein, and Edward Loper, “Natural Language Processing with Python”, O’Reilly, 2009.
4. Christopher D. Manning, Hinrich Schutze, Foundations of Statistical Natural Language Processing, MIT Press, 1999.
5. Nitin Indurkhya, Fred J. Damerau, Handbook of Natural Language Processing, 2nd Edition, CRC Press, 2010.
