* 
:properties:
:author: D. Thenmozhi
:date: 13 May 2022
:end:

#+startup: showall
{{{title-tab}}}
| CODE | COURSE TITLE  | L | T | P | C |
|      | DEEP LEARNING | 2 | 0 | 2 | 3 |

** COURSE OBJECTIVES
- To understand basics of deep neural networks
- To understand CNN and RNN architectures of deep neural networks
- To comprehend advanced deep learning models
- To learn deep learning algorithms and their applications to solve real world problems

{{{unit}}}
|UNIT I|DEEP NETWORKS BASICS|6| 
Linear Algebra: Scalars -- Vectors -- Matrices and tensors;
Probability Distributions; Gradient-based Optimization; Machine
Learning Basics: Capacity -- Overfitting and underfitting --
Hyperparameters and validation sets -- Estimators -- Bias and variance
-- Stochastic gradient descent -- Challenges motivating deep learning;
Deep Networks: Deep feedforward networks

{{{unit}}}
|UNIT II|CONVOLUTIONAL NEURAL NETWORKS|6| 
Convolution Operation -- Sparse Interactions -- Parameter Sharing --
Equivariance -- Pooling -- Convolution Variants: Strided -- Tiled --
Transposed and dilated convolutions; CNN Learning: Nonlinearity
Functions -- Loss Functions -- Regularization -- Optimizers --
Gradient Computation -- CNN through Visualization

{{{unit}}}
|UNIT III|RECURRENT NEURAL NETWORKS|6| 
Unfolding Graphs -- RNN Design Patterns: Acceptor -- Encoder --
Transducer; Gradient Computation -- Sequence Modeling Conditioned on
Contexts -- Bidirectional RNN -- Sequence to Sequence RNN -- Deep
Recurrent Networks -- Long Term Dependencies; Leaky Units: Skip connections and dropouts

{{{unit}}}
|UNIT IV|AUTOENCODERS AND GENERATIVE MODELS|6| 
Autoencoders: Undercomplete autoencoders -- Regularized autoencoders
-- Stochastic encoders and decoders -- Learning with autoencoders;
Deep Generative Models: Variational autoencoders -- Generative adversial networks

{{{unit}}}
|UNIT V|TRANSFORMER MODELS FOR NATURAL LANGUAGE PROCESSING|6|
The Encoder-Decoder Framework -- Attention Mechanisms -- Transfer Learning in NLP -- Hugging Face Transformers -- Transformer Applications -- The Hugging Face Ecosystem;  Transformer Anatomy: The Transformer Architecture -- The Encoder -- The Decoder; Summarization: GPT-2 -- T5

\hfill *Periods: 30*

** LIST OF EXPERIMENTS
1. XOR implementation using neural networks
2. Multi-class Classification deep neural networks
3. Digit recognition using CNN
4. Next word prediction using RNN
5. Image augmentation using GAN
6. Text summarization using T5/GPT-2

\hfill *Periods: 15*

\hfill *Total Periods: 45*

** COURSE OUTCOMES
After the completion of this course, students will be able to: 
1. Explain the basic concepts of deep neural networks (K2)
2. Apply convolution neural networks for real-world problems in image processing (K3)
3. Apply recurrent neural networks and its variants for text analysis (K3)
4. Apply generative models for data augmentation (K3)
5. Apply transformer models for natural language processing (K3)

** REFERENCES
1. Ian Goodfellow, Yoshua Bengio, Aaron Courville, ``Deep Learning'',
   MIT Press, 2016. (Units 1,2,3,4)
2. Lewis Tunstall, Leandro von Werra, Thomas Wolf, ``Natural Language
   Processing with Transformers'', O'Reilly Media, Inc.,
   February 2022. (Unit 5)
3. Seth Weidman. ``Deep Learning from Scratch: Building with Python
   from First Principles'', O'Reilly Media, Inc, September 2019.
4. Salman Khan, Hossein Rahmani, Syed Afaq Ali Shah, Mohammed
   Bennamoun, ``A Guide to Convolutional Neural Networks for Computer
   Vision'', Synthesis Lectures on Computer Vision, Morgan & Claypool
   publishers, 2018.
5. Yoav Goldberg, ``Neural Network Methods for Natural Language
   Processing'', Synthesis Lectures on Human Language Technologies,
   Morgan & Claypool publishers, 2017.
6. Santanu Pattanayak, ``Pro Deep Learning with TensorFlow: A
   Mathematical Approach to Advanced Artificial Intelligence in
   Python'', Apress, 2017.
   
